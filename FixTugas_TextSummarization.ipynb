{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FixTugas-TextSummarization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPWE0ATmfb++b+z3R0iLkUN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adhiutama17/MKBigData/blob/main/FixTugas_TextSummarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fka9KzCP8_ba"
      },
      "source": [
        "#Fix untuk Tugas MK Big Data - Text Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwmnPzIO9Jfs"
      },
      "source": [
        "## Daftar Isi\n",
        "\n",
        "\n",
        "\n",
        "*   Pendahuluan\n",
        "*   Teori\n",
        "*   Metode\n",
        "*   Uji Coba\n",
        "*   Validasi\n",
        "*   Test\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pendahuluan\n",
        "Era Internet menyediakan beragam informasi yang bisa diakses kapan pun dan dimana pun, misalnya berita elektronik, iklan, kursus online, hingga artikel-artikel ilmiah. Contoh terakhir menjadi salah satu sumber informasi yang wajib di dunia pendidikan khususnya pendidikan tinggi. Artikel ilmiah didefinisikan sebagai karya ilmu pengetahuan yang menyajikan fakta umum dan ditulis dengan metodologi penulisan yang baik dan benar. Konten dari artikel ilmiah dapat berupa penelitian, ulasan teori, hingga ide baru seseorang didasarkan suatu permasalahan. Menurut World Bank , dari tahun 2000 hingga 2018 tercatat lebih dari 2,5 juta artikel ilmiah yang terpublikasi melalui berbagai macam lembaga publikasi dengan pengelompokkan bidang tertentu. Pembaca artikel ilmiah perlu menemukan informasi dan gagasan penulis dari teks yang ditulis. Selain artikel ilmiah yang terpublikasi, terdapat juga artikel yang belum terpublikasi berupa paper, jurnal, atau artikel esai perlombaan mahasiswa."
      ],
      "metadata": {
        "id": "tQ4uU56JicZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dasar Teori\n",
        "\n",
        "###Text Summarization\n",
        "\n",
        "Text Summarization (Ringkasan Teks) merupakan teks yang terdiri atas satu atau dua kalimat yang menyampaikan informasi penting dari teks yang asli, biasanya jumlah kalimat tidak lebih dari setengah teks yang asli bahkan lebih sedikit\n",
        "\n",
        "*   Extractive\n",
        "\n",
        "mengidentifikasi bagian penting dari teks dan ditulis kembali dalam teks yang ringkas. Pendekatan ini akan bergantung pada kalimat-kalimat yang tercantum pada teks tersebut\n",
        "\n",
        "*   Abstractive\n",
        "\n",
        "mengidentifikasi dan mengintepretasikan bagian penting dari teks dengan membuat kalimat yang baru. Kalimat baru dari pendekatan ini dapat dimungkinkan tidak menggunakan kalimat yang tercantum tetapi yang lain dengan makna yang sama\n",
        "\n"
      ],
      "metadata": {
        "id": "w1DRZgFNilqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Repository File\n",
        "Tools pembuatan program ini menggunakan python dengan menghubungan repository Github"
      ],
      "metadata": {
        "id": "tq4FtBDAirBE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSE-_vpU86Cc"
      },
      "source": [
        "!git clone https://github.com/adhiutama17/MKBigData"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls MKBigData"
      ],
      "metadata": {
        "id": "NFQHRyi3rn88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#File Path untuk Teks yang akan diolah\n",
        "#Contoh file yang akan diolah telah ditempatkan di folder MKBigData/Teks/\n",
        "pdf_path = 'MKBigData/Teks/Abdul Rohman_Universitas Airlangga_Provider R_D.pdf' #file dalam bentuk PDF maka diperlukan tools pdfminer"
      ],
      "metadata": {
        "id": "jbojUsPTrswc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pre-Process - membuka file pdf\n",
        "file pdf di github akan dibuka dengan tools pdfminer.six"
      ],
      "metadata": {
        "id": "fMfwWhonQQIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pdfminer.six"
      ],
      "metadata": {
        "id": "IH-ZDD4irv-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer.pdfparser import PDFParser\n",
        "from pdfminer.pdfdocument import PDFDocument\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from pdfminer.pdfinterp import resolve1\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "file = open(pdf_path, 'rb') #membuka file pdf\n",
        "parser = PDFParser(file)\n",
        "document = PDFDocument(parser)\n",
        "\n",
        "print(resolve1(document.catalog['Pages'])['Count']) #mendapatkan jumlah halaman, karena terdapat halaman yang tidak diperlukan nantinya"
      ],
      "metadata": {
        "id": "ME9caPx6ryK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(pdf_path, hal_awal, hal_akhir):\n",
        "    \"\"\"\n",
        "    Membuat fungsi def untuk memanggil, membuka file pdf\n",
        "    Input  : pdf_path (lokasi file), halaman awal (bukan cover) dan akhir (sebelum daftar pustaka)\n",
        "    Output : kumpulan kalimat (disebut juga document)\n",
        "    \"\"\"\n",
        "    file_pdf = open(pdf_path, 'rb') #membuka file pdf\n",
        "    parser = PDFParser(file_pdf)\n",
        "    document = PDFDocument(parser)\n",
        "    file_text = extract_text(pdf_path, page_numbers = range(hal_awal,hal_akhir))\n",
        "    text_split = file_text.split(\". \")\n",
        "\n",
        "    print('Jumlah Halaman: '+resolve1(document.catalog['Pages'])['Count']) #mendapatkan jumlah halaman, karena terdapat halaman yang tidak diperlukan nantinya\n",
        "    print(text_split)"
      ],
      "metadata": {
        "id": "8SJkzkVZr21h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pre-Process - Lower, Stopword Remove dan Stemming\n",
        "\n",
        "\n",
        "*   setiap kata dalam dokumen akan dikonversi ke huruf kecil\n",
        "*   Menghapus stopword berbahasa Indonesia (menggunakan library sastrawi)\n",
        "*   Stemming (kata dasar), konversi setiap kata menjadi kata dasar\n",
        "\n"
      ],
      "metadata": {
        "id": "5ONpKVl6QeBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Menggunakan tools yang diperlukan\n",
        "import nltk\n",
        "from gensim import corpora\n",
        "from gensim.models import LsiModel\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
      ],
      "metadata": {
        "id": "FGVuP6UG8z9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Membuat list stopword berbahasa Indonesia\n",
        "stop_factory = StopWordRemoverFactory()\n",
        "#menambahkan stopword yang didapatkan dari library lainnya\n",
        "tambah_stopword = open('MKBigData/stopword-indo.txt',\"r\").readlines()\n",
        "tambah_stopword_clr = [item.replace(\"\\n\", \"\") for item in tambah_stopword]\n",
        "data_stop = stop_factory.get_stop_words() + tambah_stopword_clr\n",
        "stopword = stop_factory.create_stop_word_remover()"
      ],
      "metadata": {
        "id": "mrTqW-Um9NOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(doc_set, stopword_list):\n",
        "    \"\"\"Input  : dokumen input\n",
        "    Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
        "    Output : korpus/ teks bersih \"\"\"\n",
        "    #Membuat Stemmer\n",
        "    factory = StemmerFactory()\n",
        "    stemmer = factory.create_stemmer()\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    texts = []\n",
        "    for i in doc_set:\n",
        "      #Tokenisasi kalimat menjadi kata\n",
        "      raw = i.lower()\n",
        "      tokens = tokenizer.tokenize(raw)\n",
        "      #Menghapus stopword\n",
        "      stopped_tokens = [i for i in tokens if not i in stopword_list]\n",
        "      #stem tokens\n",
        "      stemmed_tokens = [stemmer.stem(i) for i in stopped_tokens]\n",
        "      #list token\n",
        "      texts.append(stemmed_tokens)\n",
        "    return texts"
      ],
      "metadata": {
        "id": "YdHq69eS5aGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Teori Latent Semantic Analysis\n",
        "Latent Semantic Analysis mengkoversi kumpulan kata ke dalam matriks term-document, yakni setiap baris mewakili kata-kata unik, sedangkan kolom mewakili konteks darimana kata-kata tersebut diambil"
      ],
      "metadata": {
        "id": "3d2QEwkbYhjN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mempersiapkan korpus dari teks bersih untuk di-konversi menjadi doc-term matrix"
      ],
      "metadata": {
        "id": "eB1yW4jCaIAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_corpus(doc_clean):\n",
        "    \"\"\"\n",
        "    Input  : teks yang sudah melalui pre-process\n",
        "    Purpose: membuat doc-term matrix dari korpus (teks bersih)\n",
        "    Output : term dictionary and Document Term Matrix\n",
        "    \"\"\"\n",
        "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
        "    dictionary = corpora.Dictionary(doc_clean)\n",
        "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
        "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
        "    # generate LDA model\n",
        "    return dictionary,doc_term_matrix"
      ],
      "metadata": {
        "id": "kmfU-8dYYgm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pemodelan LSA"
      ],
      "metadata": {
        "id": "ehgwcGO2rWXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
        "    \"\"\"\n",
        "    Input  : korpus/ teks bersih, jumlah topik yg dijadikan acuan, jumlah kata yg berkaitan dgn topik\n",
        "    Purpose: membuat model LSA\n",
        "    Output : model LSA\n",
        "    \"\"\"\n",
        "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
        "    # membuat model LSA\n",
        "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
        "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
        "    return lsamodel"
      ],
      "metadata": {
        "id": "uyoDPlWhrSku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):\n",
        "    \"\"\"\n",
        "    Input   : dictionary : Gensim dictionary\n",
        "              corpus : Gensim corpus\n",
        "              texts : List of input texts\n",
        "              stop : Max num of topics\n",
        "    purpose : Compute c_v coherence for various number of topics\n",
        "    Output  : model_list : List of LSA topic models\n",
        "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, stop, step):\n",
        "        # generate LSA model\n",
        "        model = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "    return model_list, coherence_values"
      ],
      "metadata": {
        "id": "yb4DL_o0cbdL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}