{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FixTugas-TextSummarization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMD3R8eV9aGclEyI9b1j13g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adhiutama17/MKBigData/blob/main/FixTugas_TextSummarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fka9KzCP8_ba"
      },
      "source": [
        "#Fix untuk Tugas MK Big Data - Text Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwmnPzIO9Jfs"
      },
      "source": [
        "## Daftar Isi\n",
        "\n",
        "\n",
        "\n",
        "*   Pendahuluan\n",
        "*   Teori\n",
        "*   Metode\n",
        "*   Uji Coba\n",
        "*   Validasi\n",
        "*   Test\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pendahuluan\n",
        "Era Internet menyediakan beragam informasi yang bisa diakses kapan pun dan dimana pun, misalnya berita elektronik, iklan, kursus online, hingga artikel-artikel ilmiah. Contoh terakhir menjadi salah satu sumber informasi yang wajib di dunia pendidikan khususnya pendidikan tinggi. Artikel ilmiah didefinisikan sebagai karya ilmu pengetahuan yang menyajikan fakta umum dan ditulis dengan metodologi penulisan yang baik dan benar. Konten dari artikel ilmiah dapat berupa penelitian, ulasan teori, hingga ide baru seseorang didasarkan suatu permasalahan. Menurut World Bank , dari tahun 2000 hingga 2018 tercatat lebih dari 2,5 juta artikel ilmiah yang terpublikasi melalui berbagai macam lembaga publikasi dengan pengelompokkan bidang tertentu. Pembaca artikel ilmiah perlu menemukan informasi dan gagasan penulis dari teks yang ditulis. Selain artikel ilmiah yang terpublikasi, terdapat juga artikel yang belum terpublikasi berupa paper, jurnal, atau artikel esai perlombaan mahasiswa."
      ],
      "metadata": {
        "id": "tQ4uU56JicZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dasar Teori\n",
        "\n",
        "###Text Summarization\n",
        "\n",
        "Text Summarization (Ringkasan Teks) merupakan teks yang terdiri atas satu atau dua kalimat yang menyampaikan informasi penting dari teks yang asli, biasanya jumlah kalimat tidak lebih dari setengah teks yang asli bahkan lebih sedikit\n",
        "\n",
        "*   Extractive\n",
        "\n",
        "mengidentifikasi bagian penting dari teks dan ditulis kembali dalam teks yang ringkas. Pendekatan ini akan bergantung pada kalimat-kalimat yang tercantum pada teks tersebut\n",
        "\n",
        "*   Abstractive\n",
        "\n",
        "mengidentifikasi dan mengintepretasikan bagian penting dari teks dengan membuat kalimat yang baru. Kalimat baru dari pendekatan ini dapat dimungkinkan tidak menggunakan kalimat yang tercantum tetapi yang lain dengan makna yang sama\n",
        "\n"
      ],
      "metadata": {
        "id": "w1DRZgFNilqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Repository File\n",
        "Tools pembuatan program ini menggunakan python dengan menghubungan repository Github"
      ],
      "metadata": {
        "id": "tq4FtBDAirBE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSE-_vpU86Cc"
      },
      "source": [
        "!git clone https://github.com/adhiutama17/MKBigData"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls MKBigData"
      ],
      "metadata": {
        "id": "NFQHRyi3rn88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#File Path untuk Teks yang akan diolah\n",
        "#Contoh file yang akan diolah telah ditempatkan di folder MKBigData/Teks/\n",
        "pdf_path = 'MKBigData/Teks/Abdul Rohman_Universitas Airlangga_Provider R_D.pdf' #file dalam bentuk PDF maka diperlukan tools pdfminer"
      ],
      "metadata": {
        "id": "jbojUsPTrswc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pdfminer.six"
      ],
      "metadata": {
        "id": "IH-ZDD4irv-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer.pdfparser import PDFParser\n",
        "from pdfminer.pdfdocument import PDFDocument\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from pdfminer.pdfinterp import resolve1\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "file = open(pdf_path, 'rb') #membuka file pdf\n",
        "parser = PDFParser(file)\n",
        "document = PDFDocument(parser)\n",
        "\n",
        "print(resolve1(document.catalog['Pages'])['Count']) #mendapatkan jumlah halaman, karena terdapat halaman yang tidak diperlukan nantinya"
      ],
      "metadata": {
        "id": "ME9caPx6ryK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(pdf_path, hal_awal, hal_akhir):\n",
        "    \"\"\"\n",
        "    Membuat fungsi def untuk memanggil, membuka file pdf\n",
        "    Input  : pdf_path (lokasi file), halaman awal (bukan cover) dan akhir (sebelum daftar pustaka)\n",
        "    Output : kumpulan kalimat (disebut juga document)\n",
        "    \"\"\"\n",
        "    file_pdf = open(pdf_path, 'rb') #membuka file pdf\n",
        "    parser = PDFParser(file_pdf)\n",
        "    document = PDFDocument(parser)\n",
        "    file_text = extract_text(pdf_path, page_numbers = range(hal_awal,hal_akhir))\n",
        "    text_split = file_text.split(\". \")\n",
        "\n",
        "    print('Jumlah Halaman: '+resolve1(document.catalog['Pages'])['Count']) #mendapatkan jumlah halaman, karena terdapat halaman yang tidak diperlukan nantinya\n",
        "    print(text_split)"
      ],
      "metadata": {
        "id": "8SJkzkVZr21h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from gensim import corpora\n",
        "from gensim.models import LsiModel\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
      ],
      "metadata": {
        "id": "FGVuP6UG8z9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_factory = StopWordRemoverFactory()\n",
        "#menambahkan stopword yang didapatkan dari library lainnya\n",
        "tambah_stopword = open('MKBigData/stopword-indo.txt',\"r\").readlines()\n",
        "tambah_stopword_clr = [item.replace(\"\\n\", \"\") for item in tambah_stopword]\n",
        "data_stop = stop_factory.get_stop_words() + tambah_stopword_clr\n",
        "stopword = stop_factory.create_stop_word_remover()\n",
        "\n"
      ],
      "metadata": {
        "id": "mrTqW-Um9NOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(doc_set, stopword_list):\n",
        "    \"\"\"Input  : document list\n",
        "    Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
        "    Output : preprocessed text \"\"\"    \n",
        "    # initialize regex tokenizer\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    # create Stemmer\n",
        "    stem_factory = StemmerFactory()\n",
        "    stemmer = factory.create_stemmer()\n",
        "    texts = []\n",
        "    # loop through document list\n",
        "    for i in doc_set:\n",
        "        # clean and tokenize document string\n",
        "        raw = i.lower()\n",
        "        tokens = tokenizer.tokenize(raw)\n",
        "        # remove stop words from tokens\n",
        "        stopped_tokens = [i for i in tokens if not i in stopword_list]\n",
        "        # stem tokens\n",
        "        stemmed_tokens = [stemmer.stem(i) for i in stopped_tokens]\n",
        "        # add tokens to list\n",
        "        texts.append(stemmed_tokens)\n",
        "    return texts"
      ],
      "metadata": {
        "id": "YdHq69eS5aGU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}